{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chicago Salary Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ready: polars 1.33.1\n"
          ]
        }
      ],
      "source": [
        "# Setup: import and ensure required libraries (polars/pandas/statsmodels/etc.).\n",
        "import sys, subprocess, importlib\n",
        "def ensure(pkg, extra=None):\n",
        "    try:\n",
        "        return importlib.import_module(pkg)\n",
        "    except ImportError:\n",
        "        base = pkg.split('.')[0]\n",
        "        pip_name = {'sklearn':'scikit-learn', 'gender_guesser':'gender-guesser'}.get(base, base)\n",
        "        to_install = [pip_name] + (extra or [])\n",
        "        subprocess.run([sys.executable, '-m', 'pip', 'install'] + to_install, check=True)\n",
        "        return importlib.import_module(pkg)\n",
        "pl = ensure('polars', ['pyarrow'])\n",
        "pd = ensure('pandas')\n",
        "np = ensure('numpy')\n",
        "sm = ensure('statsmodels.api')\n",
        "sms = ensure('statsmodels.stats.api')\n",
        "sm_diag = ensure('statsmodels.stats.diagnostic')\n",
        "sm_tools = ensure('statsmodels.tools.tools')\n",
        "seaborn = ensure('seaborn')\n",
        "gender_guesser = ensure('gender_guesser')\n",
        "from patsy import dmatrices\n",
        "# Use non-interactive backend to avoid macOS GUI warnings and speed up plotting\n",
        "import os as _os\n",
        "import matplotlib\n",
        "# Only force Agg backend when not running inside IPython/Jupyter\n",
        "_IS_IPY = False\n",
        "try:\n",
        "    from IPython import get_ipython as _get_ipy\n",
        "    _IS_IPY = bool(_get_ipy())\n",
        "except Exception:\n",
        "    _IS_IPY = False\n",
        "if not _IS_IPY:\n",
        "    matplotlib.use(_os.environ.get('MPLBACKEND', 'Agg'))\n",
        "import matplotlib.pyplot as plt\n",
        "def show_if_interactive():\n",
        "    try:\n",
        "        be = matplotlib.get_backend().lower()\n",
        "    except Exception:\n",
        "        be = 'agg'\n",
        "    if be not in ('agg','pdf','ps','svg','template'):\n",
        "        try:\n",
        "            plt.show()\n",
        "        except Exception:\n",
        "            pass\n",
        "sklearn = ensure('sklearn')\n",
        "shap = ensure('shap')\n",
        "try:\n",
        "    xgb = importlib.import_module('xgboost')\n",
        "except Exception:\n",
        "    xgb = None\n",
        "# Optional: PyTorch for GPU (MPS on Apple Silicon) detection\n",
        "try:\n",
        "    torch = ensure('torch')\n",
        "    HAS_MPS = hasattr(torch.backends, 'mps') and torch.backends.mps.is_available()\n",
        "    HAS_CUDA = torch.cuda.is_available()\n",
        "except Exception:\n",
        "    try:\n",
        "        torch = importlib.import_module('torch')\n",
        "        HAS_MPS = hasattr(torch.backends, 'mps') and torch.backends.mps.is_available()\n",
        "        HAS_CUDA = torch.cuda.is_available()\n",
        "    except Exception:\n",
        "        torch = None\n",
        "        HAS_MPS = False\n",
        "        HAS_CUDA = False\n",
        "print('Ready: polars', pl.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run configuration\n",
        "SAMPLE_N = None   # set to None for full dataset\n",
        "RUN_HEAVY = True  # enable heavy analyses (Oaxaca/PS/quantile) when N is sufficient\n",
        "# Accuracy/speed tradeoffs for model-based parts (SHAP etc.)\n",
        "HIGH_ACCURACY = True\n",
        "SHAP_EVAL_N = 10000 if HIGH_ACCURACY else 5000\n",
        "XGB_N_EST = 900 if HIGH_ACCURACY else 400\n",
        "XGB_LR = 0.05 if HIGH_ACCURACY else 0.1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "Expected data/chicago_salaries.csv",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m DATA = Path(\u001b[33m'\u001b[39m\u001b[33mdata/chicago_salaries.csv\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m DATA.exists():\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mExpected data/chicago_salaries.csv\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m      7\u001b[39m     df = pl.read_csv(\u001b[38;5;28mstr\u001b[39m(DATA), try_parse_dates=\u001b[38;5;28;01mFalse\u001b[39;00m, infer_schema_length=\u001b[32m1000\u001b[39m)\n",
            "\u001b[31mFileNotFoundError\u001b[39m: Expected data/chicago_salaries.csv"
          ]
        }
      ],
      "source": [
        "# Load data: read Chicago salaries CSV with Polars (full dataset; no sampling).\n",
        "from pathlib import Path\n",
        "DATA = Path('data/chicago_salaries.csv')\n",
        "if not DATA.exists():\n",
        "    raise FileNotFoundError('Expected data/chicago_salaries.csv')\n",
        "try:\n",
        "    df = pl.read_csv(str(DATA), try_parse_dates=False, infer_schema_length=1000)\n",
        "except Exception:\n",
        "    df = pl.read_csv(str(DATA), try_parse_dates=False, infer_schema_length=1000, encoding='utf8-lossy')\n",
        "# Subsample for quick compile/run\n",
        "# df = df.sample(n=1000, with_replacement=False, shuffle=True, seed=42)\n",
        "df.shape, df.columns\n",
        "# Subsample for quick compile/run (uses SAMPLE_N)\n",
        "# try:\n",
        "#     SAMPLE_N\n",
        "# except NameError:\n",
        "#     SAMPLE_N = 1000\n",
        "# if SAMPLE_N:\n",
        "#     df = df.sample(n=min(SAMPLE_N, df.height), with_replacement=False, shuffle=True, seed=42)\n",
        "# print('Using sample size:', len(df))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature engineering: normalize text, infer job_level/job_family, compute annualized pay, derive log_annual, and infer gender via gender_guesser.\n",
        "import re\n",
        "roman_map = {\n",
        "    'I':1,'II':2,'III':3,'IV':4,'V':5,'VI':6,'VII':7,'VIII':8,'IX':9,'X':10\n",
        "}\n",
        "def extract_job_level(title: str) -> int:\n",
        "    if title is None:\n",
        "        return 0\n",
        "    m = re.search(r'(?:\\b| )(?:(?:I{1,3})|IV|V|VI{0,3}|VII|VIII|IX|X)(?:\\b)', title)\n",
        "    if m:\n",
        "        return roman_map.get(m.group(0).strip(), 0)\n",
        "    return 0\n",
        "\n",
        "def simplify_job_family(title: str) -> str:\n",
        "    if not title:\n",
        "        return 'UNKNOWN'\n",
        "    # Take part before ' (' or first comma; keep first 2 words\n",
        "    base = re.split(r'\\s*\\(|,', title)[0]\n",
        "    parts = base.strip().split()[:3]\n",
        "    return ' '.join(parts) if parts else 'UNKNOWN'\n",
        "\n",
        "def to_float(s: str) -> float:\n",
        "    if s is None:\n",
        "        return None\n",
        "    s2 = re.sub(r'[^0-9.]+', '', str(s))\n",
        "    try:\n",
        "        return float(s2) if s2 else None\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# Normalize text columns\n",
        "df = df.with_columns([\n",
        "    pl.col('Name').cast(pl.Utf8).str.strip_chars().str.to_uppercase().alias('name'),\n",
        "    pl.col('JobTitle').cast(pl.Utf8).str.strip_chars().str.to_uppercase().alias('job_title'),\n",
        "    pl.col('Department').cast(pl.Utf8).str.strip_chars().str.to_uppercase().alias('department'),\n",
        "    pl.col('Time').cast(pl.Utf8).str.strip_chars().str.to_uppercase().alias('time'),\n",
        "    pl.col('PayType').cast(pl.Utf8).str.strip_chars().str.to_uppercase().alias('pay_type'),\n",
        "]).drop(['Name','JobTitle','Department','Time','PayType'])\n",
        "\n",
        "\n",
        "\n",
        "# Extract first name from 'LAST, FIRST MI' and infer gender via gender_guesser\n",
        "def extract_first_name(full: str) -> str:\n",
        "    if not full: return None\n",
        "    parts = full.split(',')\n",
        "    rhs = parts[1].strip() if len(parts) >= 2 else full\n",
        "    token = rhs.split()\n",
        "    return token[0].strip(\" . '\") if token else None\n",
        "\n",
        "names_py = df.select(pl.col('name')).to_series().to_list()\n",
        "firsts_py = [extract_first_name(n) for n in names_py]\n",
        "from gender_guesser import detector as _gd\n",
        "_det = _gd.Detector(case_sensitive=False)\n",
        "# Save gender corpus info (package version and data file hints)\n",
        "import os, importlib, sys\n",
        "os.makedirs('artifacts', exist_ok=True)\n",
        "try:\n",
        "    pkg = importlib.import_module('gender_guesser')\n",
        "    try:\n",
        "        import importlib.metadata as md\n",
        "        ver = md.version('gender-guesser')\n",
        "    except Exception:\n",
        "        ver = getattr(pkg, '__version__', 'unknown')\n",
        "    pkg_path = os.path.dirname(pkg.__file__) if hasattr(pkg,'__file__') else 'unknown'\n",
        "    # scan for likely dictionary files\n",
        "    data_files = []\n",
        "    if pkg_path != 'unknown':\n",
        "        for root, dirs, files in os.walk(pkg_path):\n",
        "            for f in files:\n",
        "                if ('nam' in f.lower() or 'dict' in f.lower()) and f.lower().endswith('.txt'):\n",
        "                    data_files.append(os.path.join(root,f))\n",
        "    with open('artifacts/gender_corpus_info.txt','w') as fh:\n",
        "        fh.write(f'gender_guesser version: {ver}\\n')\n",
        "        fh.write(f'package path: {pkg_path}\\n')\n",
        "        fh.write('candidate data files:\\n')\n",
        "        for pth in data_files[:10]:\n",
        "            fh.write('  - ' + pth + '\\n')\n",
        "except Exception as e:\n",
        "    open('artifacts/gender_corpus_info.txt','w').write('Unable to determine corpus info: ' + str(e))\n",
        "raw_gender = [_det.get_gender(fn or '') for fn in firsts_py]\n",
        "def map_gender(g):\n",
        "    if g in ('male','mostly_male'): return 'MALE'\n",
        "    if g in ('female','mostly_female'): return 'FEMALE'\n",
        "    if g == 'andy': return 'ANDROGYNOUS'\n",
        "    return 'UNKNOWN'\n",
        "gender_cat = [map_gender(g) for g in raw_gender]\n",
        "gender_bin_male = [1 if v=='MALE' else (0 if v=='FEMALE' else None) for v in gender_cat]\n",
        "df = df.with_columns([\n",
        "    pl.Series('first_name', firsts_py, dtype=pl.Utf8),\n",
        "    pl.Series('gender_guess', gender_cat, dtype=pl.Utf8),\n",
        "    pl.Series('is_male', gender_bin_male, dtype=pl.Int64)\n",
        "])\n",
        "\n",
        "# Clean numeric columns\n",
        "salary_clean = pl.col('Salary').cast(pl.Utf8).str.replace_all(r\"[^0-9.]\", \"\")\n",
        "rate_clean   = pl.col('Rate').cast(pl.Utf8).str.replace_all(r\"[^0-9.]\", \"\")\n",
        "df = df.with_columns([\n",
        "    pl.col('Hours').cast(pl.Float64, strict=False).alias('hours'),\n",
        "    pl.when(salary_clean.str.len_chars() == 0).then(None).otherwise(salary_clean.cast(pl.Float64, strict=False)).alias('salary_num'),\n",
        "    pl.when(rate_clean.str.len_chars() == 0).then(None).otherwise(rate_clean.cast(pl.Float64, strict=False)).alias('rate_num'),\n",
        "])\n",
        "\n",
        "# Derived features\n",
        "df = df.with_columns([\n",
        "    (pl.col('time') == 'F').alias('is_full_time'),\n",
        "    (pl.col('pay_type') == 'SALARY').alias('is_salary'),\n",
        "])\n",
        "\n",
        "# Job level and family (via apply to avoid complex vectorized regex)\n",
        "job_levels = df.select(pl.col('job_title')).to_series().to_list()\n",
        "levels_py = [extract_job_level(t) for t in job_levels]\n",
        "families_py = [simplify_job_family(t) for t in job_levels]\n",
        "df = df.with_columns([\n",
        "    pl.Series('job_level', levels_py, dtype=pl.Int64),\n",
        "    pl.Series('job_family', families_py, dtype=pl.Utf8).str.to_uppercase()\n",
        "])\n",
        "\n",
        "# Annualized pay\n",
        "hours_proxy = pl.when(pl.col('hours').is_not_null()).then(pl.col('hours'))\\\n",
        "    .when(pl.col('is_full_time')).then(pl.lit(40.0))\\\n",
        "    .otherwise(pl.lit(20.0))\n",
        "annual_from_hourly = (pl.col('rate_num') * hours_proxy * pl.lit(52.0))\n",
        "annualized = pl.when(pl.col('is_salary')).then(pl.col('salary_num'))\\\n",
        "    .when(pl.col('rate_num').is_not_null()).then(annual_from_hourly)\\\n",
        "    .otherwise(pl.col('salary_num'))\n",
        "df = df.with_columns([\n",
        "    annualized.alias('annualized_pay'),\n",
        "    pl.when(annualized > 0).then(annualized.log()).otherwise(None).alias('log_annual')\n",
        "])\n",
        "\n",
        "\n",
        "df.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Missingness summary\n",
        "missing = (df.select([pl.col(c).is_null().sum().alias(c) for c in df.columns])\n",
        "             .transpose(include_header=True, header_name='column', column_names=['n_missing']))\n",
        "display(missing.sort('n_missing', descending=True).head(20))\n",
        "# Distribution\n",
        "pdf = df.select(['annualized_pay','log_annual','department','job_family','is_full_time']).to_pandas()\n",
        "fig, ax = plt.subplots(1,2, figsize=(12,4))\n",
        "seaborn.histplot(data=pdf, x='annualized_pay', ax=ax[0], bins=50)\n",
        "ax[0].set_title('Annualized pay (raw)')\n",
        "seaborn.histplot(data=pdf, x='log_annual', ax=ax[1], bins=50)\n",
        "ax[1].set_title('Annualized pay (log)')\n",
        "plt.tight_layout()\n",
        "import os\n",
        "os.makedirs('artifacts', exist_ok=True)\n",
        "fig.savefig('artifacts/pay_distribution_hist.png', dpi=200, bbox_inches='tight')\n",
        "show_if_interactive()\n",
        "# Top departments/job families\n",
        "top_depts = (df.group_by('department').len().sort('len', descending=True).head(10))\n",
        "top_fams = (df.group_by('job_family').len().sort('len', descending=True).head(10))\n",
        "display(top_depts)\n",
        "display(top_fams)\n",
        "try:\n",
        "    top_depts.write_csv('artifacts/top_departments.csv')\n",
        "    top_fams.write_csv('artifacts/top_job_families.csv')\n",
        "    # Save quick bar figures for top categories\n",
        "    tdp = top_depts.to_pandas(); tfp = top_fams.to_pandas()\n",
        "    fig2, ax2 = plt.subplots(1,2, figsize=(14,4))\n",
        "    seaborn.barplot(x='len', y='department', data=tdp, ax=ax2[0])\n",
        "    ax2[0].set_title('Top Departments by Count')\n",
        "    seaborn.barplot(x='len', y='job_family', data=tfp, ax=ax2[1])\n",
        "    ax2[1].set_title('Top Job Families by Count')\n",
        "    fig2.tight_layout(); fig2.savefig('artifacts/top_categories_bars.png', dpi=200, bbox_inches='tight')\n",
        "    plt.close(fig2)\n",
        "except Exception:\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Counts\n",
        "gender_counts = df.group_by('gender_guess').len().sort('len', descending=True)\n",
        "display(gender_counts)\n",
        "\n",
        "# Pay by gender (median and IQR on annualized pay)\n",
        "gb = (df.select(['gender_guess','annualized_pay']).drop_nulls().group_by('gender_guess')\n",
        "      .agg([pl.col('annualized_pay').median().alias('median_pay'),\n",
        "            (pl.col('annualized_pay').quantile(0.75)-pl.col('annualized_pay').quantile(0.25)).alias('IQR')])\n",
        "      .sort('median_pay', descending=True))\n",
        "display(gb)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Prepare data for statsmodels (dynamic hours basis with fallback)\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', message='covariance of constraints does not have full rank', module='statsmodels')\n",
        "warnings.filterwarnings('ignore', category=RuntimeWarning, module='statsmodels')\n",
        "model_df = df.select(['log_annual','department','job_family','gender_guess','is_salary','job_level','hours']).drop_nulls(subset=['log_annual','department','job_family','gender_guess','is_salary','job_level','hours']).to_pandas()\n",
        "model_df['department'] = model_df['department'].astype('category')\n",
        "model_df['job_family'] = model_df['job_family'].astype('category')\n",
        "# Cap high-cardinality categoricals to reduce singular design matrices\n",
        "import pandas as pd, numpy as np\n",
        "\n",
        "def cap_top_n(s, n=10):\n",
        "    vc = s.value_counts(dropna=False)\n",
        "    keep = set(vc.head(n).index.tolist())\n",
        "    return s.where(s.isin(keep), other='OTHER')\n",
        "model_df['department_cap'] = cap_top_n(model_df['department'].astype(str), 10).astype('category')\n",
        "model_df['job_family_cap'] = cap_top_n(model_df['job_family'].astype(str), 10).astype('category')\n",
        "# Within-group centered hours (reduces collinearity with FE dummies)\n",
        "model_df['hours_w'] = model_df.groupby(['department_cap','job_family_cap'], observed=True)['hours'].transform(lambda s: s - s.mean())\n",
        "# Use binary male indicator to avoid redundant gender dummies\n",
        "model_df['male01'] = (model_df['gender_guess'] == 'MALE').astype(int)\n",
        "\n",
        "from patsy import dmatrices, bs\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from numpy.polynomial.legendre import legvander\n",
        "\n",
        "# Helper to add Legendre orthogonal polynomial features for hours\n",
        "def add_legendre_hours(df, col='hours_w', deg=3, prefix='hoursW_L'):\n",
        "    x = pd.to_numeric(df[col], errors='coerce')\n",
        "    # Map to [-1,1] for Legendre\n",
        "    xmin, xmax = x.min(), x.max()\n",
        "    z = 2*(x - xmin)/(xmax - xmin) - 1\n",
        "    V = legvander(z.fillna(z.mean()), deg)  # includes degree 0 column (const)\n",
        "    for d in range(1, deg+1):\n",
        "        df[f\"{prefix}{d}\"] = V[:, d]\n",
        "    return df\n",
        "\n",
        "# Formulas\n",
        "f_bs   = 'log_annual ~ male01 + C(department_cap) + C(job_family_cap) + job_level + bs(hours_w, df=5, include_intercept=False)'\n",
        "f_leg  = 'log_annual ~ male01 + C(department_cap) + C(job_family_cap) + job_level + hoursW_L1 + hoursW_L2 + hoursW_L3'\n",
        "\n",
        "used = 'bs'\n",
        "try:\n",
        "    y, X = dmatrices(f_bs, data=model_df, return_type='dataframe')\n",
        "    ols = sm.OLS(y, X).fit()\n",
        "    exog = np.asarray(X)\n",
        "    rank = int(np.linalg.matrix_rank(exog))\n",
        "    ncols = X.shape[1]\n",
        "    vifs = []\n",
        "    for j in range(ncols):\n",
        "        try:\n",
        "            vifs.append(float(variance_inflation_factor(exog, j)))\n",
        "        except Exception:\n",
        "            vifs.append(np.inf)\n",
        "    bad = (rank < ncols) or any((not np.isfinite(v)) or (v>50) for v in vifs)\n",
        "    if bad:\n",
        "        print('Switching to Legendre orthogonal polynomials for hours (spline basis caused rank/VIF issues).')\n",
        "        model_df = add_legendre_hours(model_df, 'hours', 3)\n",
        "        y, X = dmatrices(f_leg, data=model_df, return_type='dataframe')\n",
        "        ols = sm.OLS(y, X).fit()\n",
        "        used = 'legendre'\n",
        "except Exception as e:\n",
        "    print('Spline fit failed ({}). Falling back to Legendre basis.'.format(e))\n",
        "    model_df = add_legendre_hours(model_df, 'hours', 3)\n",
        "    y, X = dmatrices(f_leg, data=model_df, return_type='dataframe')\n",
        "    ols = sm.OLS(y, X).fit()\n",
        "    used = 'legendre'\n",
        "\n",
        "# Compact coefficient print to avoid summary-induced warnings\n",
        "def print_coef(res, title):\n",
        "    names = list(res.model.exog_names) if hasattr(res,'model') and hasattr(res.model,'exog_names') else []\n",
        "    p = res.params\n",
        "    if hasattr(p, 'index'):\n",
        "        idx = p.index; coef_series = p\n",
        "    else:\n",
        "        idx = names if names else [f'X{i}' for i in range(len(p))]; coef_series = pd.Series(p, index=idx)\n",
        "    bs_ = res.bse if hasattr(res,'bse') else None\n",
        "    se_series = bs_ if hasattr(bs_, 'index') else pd.Series(bs_, index=idx) if bs_ is not None else pd.Series(np.nan, index=idx)\n",
        "    tvals = coef_series / se_series\n",
        "    pvals = res.pvalues if hasattr(res,'pvalues') else pd.Series(np.nan, index=idx)\n",
        "    out = pd.DataFrame({'coef': coef_series, 'se': se_series, 't': tvals, 'pval': pvals})\n",
        "    print(title)\n",
        "    print(out.head(30).to_string())\n",
        "\n",
        "print('Hours basis used:', used)\n",
        "print_coef(ols, 'OLS (non-robust, compact)')\n",
        "\n",
        "# Heteroskedasticity tests\n",
        "from statsmodels.stats.diagnostic import het_breuschpagan, het_white\n",
        "bp_lm, bp_p, fval, f_p = het_breuschpagan(ols.resid, X)\n",
        "w_stat, w_p, f_w, f_wp = het_white(ols.resid, X)\n",
        "print('Breusch-Pagan: LM p={:.4g}, F p={:.4g}'.format(bp_p, f_p))\n",
        "print('White test: stat p={:.4g}, F p={:.4g}'.format(w_p, f_wp))\n",
        "\n",
        "# Robust SEs (HC1)\n",
        "ols_hc = ols.get_robustcov_results(cov_type='HC1')\n",
        "print_coef(ols_hc, 'OLS (HC1 robust, compact)')\n",
        "\n",
        "# Cluster-robust by department (guarded)\n",
        "clusters = model_df['department'].cat.codes.values\n",
        "try:\n",
        "    if len(clusters)==len(ols.resid) and len(set(clusters))>1:\n",
        "        ols_cl = ols.get_robustcov_results(cov_type='cluster', groups=clusters)\n",
        "        print_coef(ols_cl, 'OLS (cluster-robust, compact)')\n",
        "    else:\n",
        "        print('Skipping cluster-robust SEs (insufficient groups or length mismatch).')\n",
        "except Exception as e:\n",
        "    print('Cluster-robust failed:', e)\n",
        "# Durbin-Watson for residual independence\n",
        "from statsmodels.stats.stattools import durbin_watson\n",
        "print('Durbin-Watson:', durbin_watson(ols.resid))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Regression diagnostics: Linearity, Independence, Homoscedasticity, Normality, Multicollinearity\n",
        "import numpy as np, pandas as pd\n",
        "from statsmodels.stats.diagnostic import het_breuschpagan, het_white, acorr_breusch_godfrey, linear_reset, acorr_ljungbox\n",
        "from statsmodels.stats.stattools import jarque_bera, durbin_watson\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# Choose a fitted model to diagnose\n",
        "res = None\n",
        "for name in ['ols', 'm3_ols', 'm4_ols', 'm3', 'm4']:\n",
        "    if name in globals():\n",
        "        res = globals()[name]\n",
        "        break\n",
        "if res is None:\n",
        "    raise RuntimeError('No fitted model found for diagnostics (expected variables like ols, m3_ols, m4_ols).')\n",
        "\n",
        "# Extract design and residuals\n",
        "exog = np.asarray(res.model.exog)\n",
        "endog = np.asarray(res.model.endog).ravel()\n",
        "resid = np.asarray(res.resid).ravel()\n",
        "colnames = list(getattr(res.model, 'exog_names', [f'X{i}' for i in range(exog.shape[1])]))\n",
        "\n",
        "report = {}\n",
        "\n",
        "# 1) Linearity (RESET test)\n",
        "try:\n",
        "    reset = linear_reset(res, power=2, use_f=True)\n",
        "    # reset is a ContrastResults with attributes fvalue, pvalue\n",
        "    report['linearity_RESET_F'] = float(getattr(reset, 'fvalue', np.nan))\n",
        "    report['linearity_RESET_p'] = float(getattr(reset, 'pvalue', np.nan))\n",
        "except Exception as e:\n",
        "    report['linearity_RESET_F'] = np.nan\n",
        "    report['linearity_RESET_p'] = np.nan\n",
        "\n",
        "# 2) Independence (autocorrelation) — Durbin-Watson and Breusch-Godfrey lags 1..4\n",
        "try:\n",
        "    report['independence_DW'] = float(durbin_watson(resid))\n",
        "except Exception:\n",
        "    report['independence_DW'] = np.nan\n",
        "for L in [1,2,4]:\n",
        "    try:\n",
        "        lm, lmp, fval, fp = acorr_breusch_godfrey(res, nlags=L)\n",
        "        report[f'independence_BG_lags{L}_p'] = float(lmp)\n",
        "    except Exception:\n",
        "        report[f'independence_BG_lags{L}_p'] = np.nan\n",
        "\n",
        "# 3) Homoscedasticity — Breusch-Pagan and White\n",
        "try:\n",
        "    lm, lmp, fval, fp = het_breuschpagan(resid, exog)\n",
        "    report['homoscedasticity_BP_p'] = float(lmp)\n",
        "except Exception:\n",
        "    report['homoscedasticity_BP_p'] = np.nan\n",
        "try:\n",
        "    stat, pval, fstat, fpval = het_white(resid, exog)\n",
        "    report['homoscedasticity_White_p'] = float(pval)\n",
        "except Exception:\n",
        "    report['homoscedasticity_White_p'] = np.nan\n",
        "\n",
        "# 4) Normality of residuals — Jarque-Bera\n",
        "try:\n",
        "    jb_stat, jb_p, skew, kurt = jarque_bera(resid)\n",
        "    report['normality_JB_p'] = float(jb_p)\n",
        "    report['normality_skew'] = float(skew)\n",
        "    report['normality_kurtosis'] = float(kurt)\n",
        "except Exception:\n",
        "    report['normality_JB_p'] = np.nan\n",
        "\n",
        "# 5) No perfect multicollinearity — rank check, condition number, VIFs\n",
        "try:\n",
        "    rank = int(np.linalg.matrix_rank(exog))\n",
        "    report['multicollinearity_rank'] = rank\n",
        "    report['multicollinearity_ncols'] = int(exog.shape[1])\n",
        "    # Condition number (based on exog)\n",
        "    try:\n",
        "        cond = float(np.linalg.cond(exog))\n",
        "    except Exception:\n",
        "        # fall back to SVD-based\n",
        "        s = np.linalg.svd(exog, compute_uv=False)\n",
        "        cond = float(s.max() / s.min()) if np.all(s>0) else np.inf\n",
        "    report['multicollinearity_condition_number'] = cond\n",
        "except Exception:\n",
        "    report['multicollinearity_rank'] = np.nan\n",
        "    report['multicollinearity_ncols'] = exog.shape[1]\n",
        "    report['multicollinearity_condition_number'] = np.nan\n",
        "\n",
        "# Compute VIF for non-constant columns\n",
        "try:\n",
        "    # Identify constant-like columns (std ~ 0) and typical intercept names\n",
        "    mask_nonconst = np.array([np.std(exog[:, j]) > 1e-12 and (colnames[j].lower() not in ('const','intercept')) for j in range(exog.shape[1])])\n",
        "    X_vif = exog[:, mask_nonconst]\n",
        "    names_vif = [colnames[j] for j in range(exog.shape[1]) if mask_nonconst[j]]\n",
        "    vif_vals = []\n",
        "    for j in range(X_vif.shape[1]):\n",
        "        try:\n",
        "            vif_vals.append(float(variance_inflation_factor(X_vif, j)))\n",
        "        except Exception:\n",
        "            vif_vals.append(np.nan)\n",
        "    vif_tbl = pd.DataFrame({'feature': names_vif, 'VIF': vif_vals}).sort_values('VIF', ascending=False)\n",
        "except Exception:\n",
        "    vif_tbl = pd.DataFrame(columns=['feature','VIF'])\n",
        "\n",
        "# Present concise results with common decision thresholds\n",
        "alpha = 0.05\n",
        "print('Assumption checks (alpha=0.05):')\n",
        "# Linearity\n",
        "p = report.get('linearity_RESET_p', np.nan)\n",
        "print('- Linearity (RESET p):', p, '->', 'OK' if (np.isnan(p) or p>alpha) else 'Potential nonlinearity')\n",
        "# Independence\n",
        "dw = report.get('independence_DW', np.nan)\n",
        "bg1 = report.get('independence_BG_lags1_p', np.nan)\n",
        "print('- Independence: DW={:.3f} (≈2 ok); BG p(lag1)={}'.format(dw if np.isfinite(dw) else np.nan, bg1))\n",
        "# Homoscedasticity\n",
        "bp = report.get('homoscedasticity_BP_p', np.nan)\n",
        "wh = report.get('homoscedasticity_White_p', np.nan)\n",
        "print('- Homoscedasticity: BP p={}, White p={}'.format(bp, wh))\n",
        "# Normality\n",
        "jb = report.get('normality_JB_p', np.nan)\n",
        "print('- Normality (JB p):', jb)\n",
        "# Multicollinearity\n",
        "rank = report.get('multicollinearity_rank', np.nan)\n",
        "ncols = report.get('multicollinearity_ncols', np.nan)\n",
        "cond = report.get('multicollinearity_condition_number', np.nan)\n",
        "print('- Rank check: rank={} vs columns={} -> {}'.format(rank, ncols, 'Full rank' if rank==ncols else 'Rank deficient'))\n",
        "print('- Condition number (rule of thumb: < 30-50 acceptable):', cond)\n",
        "print()\n",
        "print('Top VIFs (drop const):')\n",
        "try:\n",
        "    display(vif_tbl.head(15))\n",
        "except Exception:\n",
        "    print(vif_tbl.head(15).to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Influence analysis: leverage and Cook's distance (fast + cleaner)\n",
        "import numpy as np, pandas as pd, matplotlib.pyplot as plt, os\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Select model for influence (prefer FE OLS m3 if available)\n",
        "res_inf = None\n",
        "for name in ['m3', 'ols']:\n",
        "    if name in globals():\n",
        "        res_inf = globals()[name]\n",
        "        break\n",
        "if res_inf is None:\n",
        "    raise RuntimeError('No fitted model found for influence analysis (expected m3 or ols).')\n",
        "\n",
        "# Guard against heavy influence computations on large X\n",
        "n = int(len(res_inf.model.endog))\n",
        "try:\n",
        "    p = int(res_inf.model.exog.shape[1])\n",
        "except Exception:\n",
        "    p = 0\n",
        "max_work = 8_000_000  # threshold for n*p; above this we subsample\n",
        "\n",
        "if n * max(1,p) > max_work:\n",
        "    rng = np.random.default_rng(42)\n",
        "    take = min(8000, n)\n",
        "    idx = rng.choice(n, size=take, replace=False)\n",
        "    res_sub = sm.OLS(res_inf.model.endog[idx], res_inf.model.exog[idx, :]).fit()\n",
        "    infl = res_sub.get_influence()\n",
        "    hat = np.asarray(infl.hat_matrix_diag)\n",
        "    try:\n",
        "        stud = np.asarray(infl.resid_studentized_internal)\n",
        "    except Exception:\n",
        "        stud = np.asarray(res_sub.resid)\n",
        "    cooks = np.asarray(infl.cooks_distance[0])\n",
        "    obs = idx\n",
        "else:\n",
        "    infl = res_inf.get_influence()\n",
        "    # Avoid summary_frame() to skip expensive LOO computations\n",
        "    hat = np.asarray(infl.hat_matrix_diag)\n",
        "    try:\n",
        "        stud = np.asarray(infl.resid_studentized_internal)\n",
        "    except Exception:\n",
        "        stud = np.asarray(res_inf.resid)\n",
        "    cooks = np.asarray(infl.cooks_distance[0])\n",
        "    obs = np.arange(n)\n",
        "\n",
        "os.makedirs('artifacts', exist_ok=True)\n",
        "# Save top-20 by Cook's D\n",
        "(pd.DataFrame({'obs': obs, 'hat_diag': hat, 'student_resid': stud, 'cooks_d': cooks})\n",
        "   .sort_values('cooks_d', ascending=False)\n",
        "   .head(20)\n",
        "   .to_csv('artifacts/influence_top20.csv', index=False))\n",
        "\n",
        "# Bubble scatter (size=Cooks D), annotate top-10 only\n",
        "s = 2000 * np.clip(cooks, 0, np.nanpercentile(cooks, 95))\n",
        "fig, ax = plt.subplots(1,1, figsize=(7,5))\n",
        "ax.scatter(hat, stud, s=s, alpha=0.25, edgecolor='k', linewidth=0.2)\n",
        "ax.axhline(0, color='gray', lw=0.8)\n",
        "ax.set_xlabel('Leverage (hat)'); ax.set_ylabel('Studentized Residuals')\n",
        "ax.set_title('Influence: Leverage vs Studentized Residuals (size=Cooks D)')\n",
        "lab_idx = np.argsort(-cooks)[:10]\n",
        "for i in lab_idx:\n",
        "    ax.annotate(int(obs[i]), (hat[i], stud[i]), textcoords='offset points', xytext=(5,5), fontsize=8, color='darkred')\n",
        "fig.tight_layout(); fig.savefig('artifacts/influence_scatter_clean.png', dpi=200, bbox_inches='tight'); plt.close(fig)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Residual diagnostics\n",
        "fitted = ols.fittedvalues\n",
        "resid = ols.resid\n",
        "fig, ax = plt.subplots(1,2, figsize=(12,4))\n",
        "seaborn.scatterplot(x=fitted, y=resid, ax=ax[0], s=10)\n",
        "ax[0].axhline(0, color='red', lw=1); ax[0].set_title('Residuals vs Fitted')\n",
        "import scipy.stats as stats\n",
        "stats.probplot(resid, dist='norm', plot=ax[1])\n",
        "ax[1].set_title('Normal Q-Q')\n",
        "plt.tight_layout(); show_if_interactive()\n",
        "\n",
        "# VIF for numeric regressors (updated to reflect current columns)\n",
        "import numpy as np, pandas as pd\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from statsmodels.tools.tools import add_constant\n",
        "\n",
        "num_candidates = []\n",
        "for col in ['is_salary','job_level','hours_w','hoursW_L1','hoursW_L2','hoursW_L3']:\n",
        "    if col in model_df.columns:\n",
        "        num_candidates.append(col)\n",
        "if not num_candidates:\n",
        "    print('Skipping VIF: no numeric candidate columns found in model_df.')\n",
        "else:\n",
        "    X_num = model_df[num_candidates].apply(pd.to_numeric, errors='coerce').dropna()\n",
        "    # drop constant columns\n",
        "    keep = [c for c in X_num.columns if X_num[c].std(ddof=0) > 0]\n",
        "    X_num = X_num[keep]\n",
        "    if len(keep) >= 2 and len(X_num) >= 3:\n",
        "        Xn = add_constant(X_num, has_constant='add')\n",
        "        vifs = []\n",
        "        for j, col in enumerate(Xn.columns):\n",
        "            if col == 'const':\n",
        "                continue\n",
        "            try:\n",
        "                v = variance_inflation_factor(Xn.values, j)\n",
        "                vifs.append({'feature': col, 'VIF': float(v) if np.isfinite(v) else np.nan})\n",
        "            except Exception:\n",
        "                vifs.append({'feature': col, 'VIF': np.nan})\n",
        "        vif_df = pd.DataFrame(vifs).sort_values('VIF', ascending=False)\n",
        "        display(vif_df)\n",
        "    else:\n",
        "        print('Skipping VIF: not enough variable features or rows.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "hc_table = ols_hc.summary2().tables[1]\n",
        "hc_table = hc_table.rename(columns={'Coef.':'coef','P>|t|':'pval'})\n",
        "hc_table['effect_pct'] = (np.exp(hc_table['coef']) - 1.0) * 100.0\n",
        "# Show top 10 by absolute effect (excluding intercept)\n",
        "hc_table_f = hc_table.loc[hc_table.index != 'Intercept'].copy()\n",
        "hc_table_f['abs_effect'] = hc_table_f['effect_pct'].abs()\n",
        "display(hc_table_f.sort_values('abs_effect', ascending=False).head(10)[['coef','effect_pct','pval']])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SHAP with plots (guarded, MPS-accelerated path when available)\n",
        "import numpy as np\n",
        "import shap\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "base = df.select(['log_annual','department','job_family','gender_guess','is_full_time','is_salary','job_level','hours']).drop_nulls(subset=['log_annual']).to_pandas().copy()\n",
        "# reduce cardinality\n",
        "def cap_top_n(s, n=20):\n",
        "    vc = s.value_counts(dropna=False)\n",
        "    keep = set(vc.head(n).index.tolist())\n",
        "    return s.where(s.isin(keep), other='OTHER')\n",
        "base['department_cap'] = cap_top_n(base['department'].astype(str), 20)\n",
        "base['job_family_cap'] = cap_top_n(base['job_family'].astype(str), 20)\n",
        "cat_cols = ['department_cap','job_family_cap','gender_guess']\n",
        "num_cols = ['is_full_time','is_salary','job_level','hours']\n",
        "y = base['log_annual'].values\n",
        "X = base[cat_cols+num_cols]\n",
        "if len(base) < 30:\n",
        "    print('Skipping SHAP: too few rows after filtering (', len(base), ').')\n",
        "else:\n",
        "    pre = ColumnTransformer([\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_cols),\n",
        "        ('num', 'passthrough', num_cols)\n",
        "    ])\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    USE_MPS_SHAP = bool(globals().get('HAS_MPS', False) and os.environ.get('USE_MPS_SHAP', '1') != '0')\n",
        "    model = None\n",
        "    shap_values = None\n",
        "    feature_names = None\n",
        "    X_full = None\n",
        "    if USE_MPS_SHAP:\n",
        "        try:\n",
        "            import torch\n",
        "            import torch.nn as nn\n",
        "            import torch.optim as optim\n",
        "            DEVICE = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
        "            Xtr = pre.fit_transform(X_train).astype(np.float32)\n",
        "            Xte = pre.transform(X_test).astype(np.float32)\n",
        "            X_full = pre.transform(X).astype(np.float32)\n",
        "            feature_names = pre.get_feature_names_out().tolist()\n",
        "            ytr = y_train.astype(np.float32)\n",
        "            yte = y_test.astype(np.float32)\n",
        "            class MLP(nn.Module):\n",
        "                def __init__(self, in_dim):\n",
        "                    super().__init__()\n",
        "                    self.net = nn.Sequential(\n",
        "                        nn.Linear(in_dim, 256), nn.ReLU(),\n",
        "                        nn.Linear(256, 128), nn.ReLU(),\n",
        "                        nn.Linear(128, 1)\n",
        "                    )\n",
        "                def forward(self, x):\n",
        "                    return self.net(x)\n",
        "            mlp = MLP(Xtr.shape[1]).to(DEVICE)\n",
        "            opt = optim.Adam(mlp.parameters(), lr=1e-3)\n",
        "            lossf = nn.MSELoss()\n",
        "            xb = torch.from_numpy(Xtr).to(DEVICE)\n",
        "            yb = torch.from_numpy(ytr).view(-1,1).to(DEVICE)\n",
        "            mlp.train()\n",
        "            for _ in range(80):\n",
        "                opt.zero_grad(); pred = mlp(xb); loss = lossf(pred, yb); loss.backward(); opt.step()\n",
        "            mlp.eval()\n",
        "            with torch.no_grad():\n",
        "                r2_tr = 1.0 - float(((mlp(xb)-yb)**2).mean().cpu().item())/float(np.var(ytr)) if np.var(ytr)>0 else float('nan')\n",
        "                r2_te = 1.0 - float(((mlp(torch.from_numpy(Xte).to(DEVICE))-torch.from_numpy(yte).view(-1,1).to(DEVICE))**2).mean().cpu().item())/float(np.var(yte)) if np.var(yte)>0 else float('nan')\n",
        "            print('SHAP MLP (MPS) R2 train:', r2_tr, 'val:', r2_te)\n",
        "            # SHAP DeepExplainer for PyTorch\n",
        "            rng = np.random.RandomState(42)\n",
        "            bg_idx = rng.choice(len(X_full), size=min(200, len(X_full)), replace=False)\n",
        "            background = torch.from_numpy(X_full[bg_idx]).to(DEVICE)\n",
        "            eval_idx = rng.choice(len(X_full), size=min(1500, len(X_full)), replace=False)\n",
        "            X_eval = torch.from_numpy(X_full[eval_idx]).to(DEVICE)\n",
        "            explainer = shap.DeepExplainer(mlp, background)\n",
        "            sv = explainer.shap_values(X_eval)\n",
        "            sv_np = sv[0].detach().cpu().numpy() if isinstance(sv, (list, tuple)) else sv.detach().cpu().numpy()\n",
        "            shap_values = sv_np\n",
        "            X_full = X_full[eval_idx]\n",
        "            SHAP_INFO = {'model': 'mlp_mps', 'n_eval': int(X_full.shape[0])}\n",
        "        except Exception as e:\n",
        "            print('SHAP MLP (MPS) failed:', e, '— falling back to CPU tree model.')\n",
        "            USE_MPS_SHAP = False\n",
        "    if not USE_MPS_SHAP:\n",
        "        try:\n",
        "            import xgboost as _xgb\n",
        "            params = dict(n_estimators=globals().get('XGB_N_EST', 400), max_depth=8,\n",
        "                          learning_rate=globals().get('XGB_LR', 0.1), subsample=0.8,\n",
        "                          colsample_bytree=0.8, reg_lambda=1.0, min_child_weight=1.0,\n",
        "                          tree_method='hist', n_jobs=-1, random_state=42)\n",
        "            if 'HAS_CUDA' in globals() and HAS_CUDA:\n",
        "                params['tree_method'] = 'gpu_hist'\n",
        "            model = _xgb.XGBRegressor(**params)\n",
        "            shap_model_name = 'xgb_gpu' if params.get('tree_method')=='gpu_hist' else 'xgb_cpu'\n",
        "        except Exception:\n",
        "            model = RandomForestRegressor(n_estimators=300 if globals().get('HIGH_ACCURACY', False) else 200, max_depth=10 if globals().get('HIGH_ACCURACY', False) else 8, random_state=42, n_jobs=-1)\n",
        "            shap_model_name = 'rf_cpu'\n",
        "        pipe = Pipeline(steps=[('pre', pre), ('model', model)])\n",
        "        pipe.fit(X_train, y_train)\n",
        "        pred = pipe.predict(X_test)\n",
        "        print('R2 train:', r2_score(y_train, pipe.predict(X_train)), 'val R2:', r2_score(y_test, pred))\n",
        "        pre_fitted = pipe.named_steps['pre']\n",
        "        feature_names = pre_fitted.get_feature_names_out().tolist()\n",
        "        X_full = pre_fitted.transform(X)\n",
        "        import numpy as _np\n",
        "        X_full = _np.asarray(X_full, dtype=float)\n",
        "        rng = np.random.RandomState(42)\n",
        "        eval_idx = rng.choice(X_full.shape[0], size=min(int(globals().get('SHAP_EVAL_N', 5000)), X_full.shape[0]), replace=False)\n",
        "        bg_idx = rng.choice(X_full.shape[0], size=min(1000, X_full.shape[0]), replace=False)\n",
        "        X_eval = X_full[eval_idx]\n",
        "        X_bg = X_full[bg_idx]\n",
        "        try:\n",
        "            explainer = shap.TreeExplainer(pipe.named_steps['model'], data=X_bg, feature_perturbation='interventional')\n",
        "            shap_values = explainer.shap_values(X_eval)\n",
        "        except Exception:\n",
        "            explainer = shap.Explainer(pipe.named_steps['model'], X_bg)\n",
        "            sv = explainer(X_eval)\n",
        "            shap_values = sv.values if hasattr(sv, 'values') else sv\n",
        "        X_full = X_eval\n",
        "        SHAP_INFO = {'model': shap_model_name, 'n_eval': int(X_full.shape[0])}\n",
        "\n",
        "    # SHAP plots (beeswarm and bar)\n",
        "    shap.summary_plot(shap_values, X_full, feature_names=feature_names, show=False)\n",
        "    os.makedirs('artifacts', exist_ok=True)\n",
        "    plt.savefig('artifacts/shap_summary_beeswarm.png', dpi=200, bbox_inches='tight')\n",
        "    plt.title('SHAP Summary (beeswarm): contribution to log(annualized pay)')\n",
        "    show_if_interactive()\n",
        "    shap.summary_plot(shap_values, X_full, feature_names=feature_names, plot_type='bar', show=False)\n",
        "    plt.savefig('artifacts/shap_importance_bar.png', dpi=200, bbox_inches='tight')\n",
        "    plt.title('Mean |SHAP| values (global importance)')\n",
        "    show_if_interactive()\n",
        "    # Dependence plots for top 3 features\n",
        "    mean_abs = np.abs(shap_values).mean(axis=0)\n",
        "    top_idx = np.argsort(-mean_abs)[:3]\n",
        "    for idx in top_idx:\n",
        "        feat = feature_names[idx]\n",
        "        shap.dependence_plot(feat, shap_values, X_full, feature_names=feature_names, show=False)\n",
        "        os.makedirs('artifacts', exist_ok=True)\n",
        "        safe = ''.join(c if c.isalnum() or c in '._-' else '_' for c in feat)[:80]\n",
        "        plt.savefig(f'artifacts/shap_dependence_{safe}.png', dpi=200, bbox_inches='tight')\n",
        "        plt.title(f'SHAP dependence: {feat}')\n",
        "        show_if_interactive()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summarize SHAP importances if available\n",
        "try:\n",
        "    import numpy as np, pandas as pd\n",
        "    mean_abs = np.abs(shap_values).mean(axis=0)\n",
        "    mean_signed = shap_values.mean(axis=0)\n",
        "    imp_tbl = pd.DataFrame({'feature': feature_names, 'mean_abs_shap': mean_abs, 'mean_signed_shap': mean_signed}).sort_values('mean_abs_shap', ascending=False)\n",
        "    display(imp_tbl.head(20))\n",
        "    print('Top 10 features:')\n",
        "    for _, row in imp_tbl.head(10).iterrows():\n",
        "        trend = 'increase' if row['mean_signed_shap'] > 0 else 'decrease'\n",
        "        print('- {}: mean SHAP={:.4f}, tends to {} log pay'.format(row['feature'], row['mean_signed_shap'], trend))\n",
        "except Exception as e:\n",
        "    print('SHAP summary unavailable:', e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd, numpy as np\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.formula.api as smf\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# Focus on observations with MALE or FEMALE only\n",
        "gdf = df.filter(pl.col('gender_guess').is_in(['MALE','FEMALE']))\n",
        "gdf = gdf.drop_nulls(subset=['log_annual'])\n",
        "gpd = gdf.select(['log_annual','annualized_pay','gender_guess','hours','job_level','is_full_time','is_salary','department','job_family']).to_pandas()\n",
        "gpd['male01'] = (gpd['gender_guess'] == 'MALE').astype(int)\n",
        "# Basic shapes\n",
        "print('N (male,female)=', gpd['male01'].sum(), (1-gpd['male01']).sum())\n",
        "print('Overall N=', len(gpd))\n",
        "# Descriptives by gender\n",
        "desc = gpd.groupby('male01').agg({\n",
        "    'log_annual':['mean','std'],\n",
        "    'hours':['mean','median'],\n",
        "    'job_level':['mean','median'],\n",
        "    'is_full_time':['mean'],\n",
        "    'is_salary':['mean'],\n",
        "})\n",
        "display(desc)\n",
        "# Top departments by share within each gender\n",
        "for label,val in [('Male',1),('Female',0)]:\n",
        "    t = (gpd[gpd['male01']==val]['department'].value_counts(normalize=True).head(10)*100).round(1)\n",
        "    print(f\"Top departments for {label} (%):\\n\", t.to_string())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standardized mean differences (SMD) for select covariates\n",
        "def smd_numeric(x, g):\n",
        "    x1 = x[g==1].astype(float); x0 = x[g==0].astype(float)\n",
        "    m1, m0 = x1.mean(), x0.mean()\n",
        "    v1, v0 = x1.var(ddof=1), x0.var(ddof=1)\n",
        "    sp = np.sqrt((v1+v0)/2.0)\n",
        "    return (m1-m0)/sp if sp>0 else 0.0\n",
        "\n",
        "def smd_binary(x, g):\n",
        "    p1 = x[g==1].mean(); p0 = x[g==0].mean()\n",
        "    p = 0.5*(p1+p0)\n",
        "    sp = np.sqrt(p*(1-p))\n",
        "    return (p1-p0)/sp if sp>0 else 0.0\n",
        "\n",
        "smds = {\n",
        "    'hours': smd_numeric(gpd['hours'].fillna(gpd['hours'].median()), gpd['male01']),\n",
        "    'job_level': smd_numeric(gpd['job_level'], gpd['male01']),\n",
        "    'is_full_time': smd_binary(gpd['is_full_time'], gpd['male01']),\n",
        "    'is_salary': smd_binary(gpd['is_salary'], gpd['male01']),\n",
        "}\n",
        "print('Standardized mean differences (male - female):')\n",
        "for k,v in smds.items():\n",
        "    print(f'  {k}: {v:.3f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gender-gap models: robust for small samples and naming\n",
        "import numpy as np, pandas as pd\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.formula.api as smf\n",
        "\n",
        "def effect_pct_from_model(res, name='male01'):\n",
        "    # Robustly find the coefficient even if patsy renames it\n",
        "    names = []\n",
        "    if hasattr(res, 'model') and hasattr(res.model, 'exog_names'):\n",
        "        names = list(res.model.exog_names)\n",
        "    elif hasattr(res, 'params'):\n",
        "        p = res.params\n",
        "        names = list(p.index) if hasattr(p, 'index') else []\n",
        "    cand = None\n",
        "    for k in names:\n",
        "        if k == name or name+'[T.1]' == k or (name in str(k)):\n",
        "            cand = k; break\n",
        "    if cand is None:\n",
        "        return np.nan, np.nan, np.nan\n",
        "    # Pull est/SE, handling ndarray or Series\n",
        "    p = res.params\n",
        "    if hasattr(p, 'index'):\n",
        "        b = p[cand]\n",
        "        se = res.bse[cand] if hasattr(res, 'bse') and hasattr(res.bse, 'index') and cand in res.bse.index else np.nan\n",
        "    else:\n",
        "        try:\n",
        "            idx = names.index(cand)\n",
        "            b = p[idx]\n",
        "            se = res.bse[idx] if hasattr(res, 'bse') else np.nan\n",
        "        except Exception:\n",
        "            return np.nan, np.nan, np.nan\n",
        "    if not np.isfinite(se):\n",
        "        return np.nan, np.nan, np.nan\n",
        "    lo, hi = b - 1.96*se, b + 1.96*se\n",
        "    to_pct = lambda x: (float(np.exp(x)) - 1.0) * 100.0\n",
        "    return to_pct(b), to_pct(lo), to_pct(hi)\n",
        "\n",
        "# Build gpd (already created earlier); filter to MALE/FEMALE only\n",
        "work = gpd.copy()\n",
        "vc = work['male01'].value_counts(dropna=False)\n",
        "if vc.size < 2 or min(vc.values) < 3:\n",
        "    print('Skipping OLS male effect: insufficient variation in male01 (N by gender = {})'.format(vc.to_dict()))\n",
        "    # Welch fallback on log_annual\n",
        "    g0 = work[work['male01']==0]['log_annual']; g1 = work[work['male01']==1]['log_annual']\n",
        "    if len(g0) >= 2 and len(g1) >= 2:\n",
        "        diff = g1.mean() - g0.mean()\n",
        "        pct = (np.exp(diff)-1)*100\n",
        "        print('Welch fallback (M1): male effect ≈ {:.2f}% (no CI)'.format(pct))\n",
        "    else:\n",
        "        print('Welch fallback unavailable: groups too small.')\n",
        "else:\n",
        "    # Use full sample (no per-group downsampling)\n",
        "    # Fit OLS models with HC1 robust inference\n",
        "    m1 = smf.ols('log_annual ~ male01', data=work).fit()\n",
        "    m2 = smf.ols('log_annual ~ male01 + hours + I(hours**2) + job_level + is_salary', data=work).fit()\n",
        "    m3 = smf.ols('log_annual ~ male01 + hours + I(hours**2) + job_level + C(department) + C(job_family)', data=work).fit()\n",
        "    m4 = smf.ols('log_annual ~ male01*(hours + job_level) + C(department) + C(job_family)', data=work).fit()\n",
        "    for lab, res in [('M1 Unadj', m1), ('M2 +Hours+Job', m2), ('M3 +FE', m3), ('M4 +Inter', m4)]:\n",
        "        try:\n",
        "            hc = res.get_robustcov_results(cov_type='HC1')\n",
        "        except Exception:\n",
        "            hc = res\n",
        "        e, lo, hi = effect_pct_from_model(hc, 'male01')\n",
        "        if np.isfinite(e):\n",
        "            print('{} male effect: {:.2f}% (95% CI {:.2f}% to {:.2f}%)'.format(lab, e, lo, hi))\n",
        "        else:\n",
        "            if lab=='M1 Unadj':\n",
        "                g0 = work[work['male01']==0]['log_annual']; g1 = work[work['male01']==1]['log_annual']\n",
        "                if len(g0)>=2 and len(g1)>=2:\n",
        "                    diff = g1.mean() - g0.mean()\n",
        "                    pct = (np.exp(diff)-1)*100\n",
        "                    print('M1 Welch fallback: male effect ≈ {:.2f}% (no CI)'.format(pct))\n",
        "                else:\n",
        "                    print('M1 Welch fallback unavailable: groups too small.')\n",
        "            else:\n",
        "                print('{} male effect: unavailable (dropped or singular)'.format(lab))\n",
        "    # Additional nonlinear spec (hours^2)\n",
        "    m5 = smf.ols('log_annual ~ male01 + hours + I(hours**2) + job_level + C(department) + C(job_family)', data=work).fit()\n",
        "    for lab, res in [('M5 +FE +hours^2', m5)]:\n",
        "        try:\n",
        "            hc = res.get_robustcov_results(cov_type='HC1')\n",
        "        except Exception:\n",
        "            hc = res\n",
        "        e, lo, hi = effect_pct_from_model(hc, 'male01')\n",
        "        if np.isfinite(e):\n",
        "            print('{} male effect: {:.2f}% (95% CI {:.2f}% to {:.2f}%)'.format(lab, e, lo, hi))\n",
        "        else:\n",
        "            print('{} male effect: unavailable (dropped or singular)'.format(lab))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Oaxaca-Blinder decomposition (fallback implemented if statsmodels lacks Oaxaca)\n",
        "import numpy as np, pandas as pd\n",
        "\n",
        "# Choose covariates (numeric/binary only to avoid collinearity issues)\n",
        "feats = ['hours','job_level','is_full_time','is_salary']\n",
        "ox = gpd[['log_annual','male01'] + feats].copy()\n",
        "ox = ox.dropna(subset=['log_annual','male01'] + feats)\n",
        "ox['male01'] = ox['male01'].astype(int)\n",
        "\n",
        "try:\n",
        "    from statsmodels.stats.oaxaca import Oaxaca\n",
        "    exog = ox[feats]\n",
        "    oax = Oaxaca(ox['log_annual'], exog, ox['male01']).fit()\n",
        "    print(oax.summary())\n",
        "except Exception as e:\n",
        "    # Manual twofold Oaxaca-Blinder (female as reference)\n",
        "    from statsmodels.api import OLS, add_constant\n",
        "    d0 = ox[ox['male01']==0]\n",
        "    d1 = ox[ox['male01']==1]\n",
        "    if len(d0) < 10 or len(d1) < 10:\n",
        "        print('Oaxaca fallback: groups too small (N0={}, N1={})'.format(len(d0), len(d1)))\n",
        "    else:\n",
        "        X0 = add_constant(d0[feats].astype(float))\n",
        "        y0 = d0['log_annual'].astype(float)\n",
        "        X1 = add_constant(d1[feats].astype(float))\n",
        "        y1 = d1['log_annual'].astype(float)\n",
        "        b0 = OLS(y0, X0).fit().params\n",
        "        b1 = OLS(y1, X1).fit().params\n",
        "        # Group mean X (include const)\n",
        "        xbar0 = X0.mean(axis=0)\n",
        "        xbar1 = X1.mean(axis=0)\n",
        "        # Twofold (reference: female, group 0)\n",
        "        explained = float(((xbar1 - xbar0) * b0).sum())\n",
        "        unexplained = float((xbar1 @ b1 - xbar0 @ b0) - explained)\n",
        "        # Per-feature contributions\n",
        "        contrib_expl = ((xbar1 - xbar0) * b0).to_frame('explained')\n",
        "        # Align indices and fill missing\n",
        "        diff_b = (b1 - b0).reindex(contrib_expl.index).fillna(0.0)\n",
        "        xbar1_aligned = xbar1.reindex(contrib_expl.index).fillna(0.0)\n",
        "        contrib_unexpl = (xbar1_aligned * diff_b).to_frame('unexplained')\n",
        "        out = pd.concat([contrib_expl, contrib_unexpl], axis=1)\n",
        "        out.loc['TOTAL'] = [out['explained'].sum(), out['unexplained'].sum()]\n",
        "        print('Oaxaca-Blinder (manual twofold, ref=female):')\n",
        "        print('  Explained (endowments): {:.4f} (≈{:.2f}%)'.format(explained, (np.exp(explained)-1)*100))\n",
        "        print('  Unexplained (coefficients): {:.4f} (≈{:.2f}%)'.format(unexplained, (np.exp(unexplained)-1)*100))\n",
        "        try:\n",
        "            display(out)\n",
        "        except Exception:\n",
        "            print(out.to_string())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Propensity score weighting (ATE)\n",
        "# Always attempt weighting; use robust fallbacks for small samples or separation.\n",
        "work = gpd.copy()\n",
        "# Reduce cardinality for high-cardinality categoricals to stabilize logits\n",
        "for col in ['department','job_family']:\n",
        "    vc = work[col].astype(str).value_counts(dropna=False)\n",
        "    keep = set(vc.head(20).index)\n",
        "    work[col] = work[col].astype(str).where(work[col].astype(str).isin(keep), other='OTHER')\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Design matrix for propensity\n",
        "base_cols = ['hours','job_level','is_salary','department','job_family']\n",
        "Xps = pd.get_dummies(work[base_cols], drop_first=True)\n",
        "Xps = Xps.apply(pd.to_numeric, errors='coerce')\n",
        "Xps = pd.DataFrame(SimpleImputer(strategy='most_frequent').fit_transform(Xps), columns=Xps.columns)\n",
        "yps = work['male01'].astype(int).values\n",
        "\n",
        "# Try to fit logistic; fallbacks for small N or separation\n",
        "lr = None\n",
        "err = None\n",
        "for solver in ['lbfgs','liblinear','saga']:\n",
        "    try:\n",
        "        lr_try = LogisticRegression(max_iter=5000, solver=solver)\n",
        "        lr_try.fit(Xps, yps)\n",
        "        lr = lr_try\n",
        "        break\n",
        "    except Exception as e:\n",
        "        err = e\n",
        "\n",
        "if lr is None:\n",
        "    # Fallback to numeric-only covariates\n",
        "    Xnum = work[['hours','job_level','is_salary']].copy()\n",
        "    Xnum = Xnum.apply(pd.to_numeric, errors='coerce')\n",
        "    Xnum = pd.DataFrame(SimpleImputer(strategy='median').fit_transform(Xnum), columns=Xnum.columns)\n",
        "    for solver in ['lbfgs','liblinear']:\n",
        "        try:\n",
        "            lr_try = LogisticRegression(max_iter=5000, solver=solver)\n",
        "            lr_try.fit(Xnum, yps)\n",
        "            lr = lr_try\n",
        "            Xps = Xnum\n",
        "            break\n",
        "        except Exception as e:\n",
        "            err = e\n",
        "\n",
        "if lr is None:\n",
        "    print('Propensity fit failed ({}). Using uniform weights; re-run with larger sample if possible.'.format(err))\n",
        "    p = np.full_like(yps, fill_value=yps.mean(), dtype=float)\n",
        "else:\n",
        "    p = lr.predict_proba(Xps)[:,1]\n",
        "\n",
        "# Clip probabilities to avoid extreme weights\n",
        "p = np.clip(p, 0.01, 0.99)\n",
        "# ATE weights (unstabilized)\n",
        "w = np.where(yps==1, 1.0/p, 1.0/(1.0-p))\n",
        "work['w'] = w\n",
        "\n",
        "# Weighted outcome model (HC1 robust)\n",
        "try:\n",
        "    wres = smf.wls('log_annual ~ male01 + hours + I(hours**2) + job_level + C(department) + C(job_family)', data=work, weights=work['w']).fit(cov_type='HC1')\n",
        "    e,lo,hi = (np.exp(wres.params['male01'])-1)*100, (np.exp(wres.conf_int().loc['male01',0])-1)*100, (np.exp(wres.conf_int().loc['male01',1])-1)*100\n",
        "    print('Weighted (ATE) male effect: {:.2f}% (95% CI {:.2f}% to {:.2f}%)'.format(e,lo,hi))\n",
        "except Exception as e:\n",
        "    print('Weighted outcome model failed:', e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quantile regression (skip on small sample to avoid instability)\n",
        "if len(gpd) < 200:\n",
        "    print('Skipping quantile regression: sample too small (N={}).'.format(len(gpd)))\n",
        "else:\n",
        "    from statsmodels.regression.quantile_regression import QuantReg\n",
        "    qr_df = gpd.copy()\n",
        "    # Reduce cardinality for high-cardinality categoricals and ensure strings\n",
        "    for col in ['department','job_family']:\n",
        "        vc = qr_df[col].astype(str).value_counts(dropna=False)\n",
        "        keep = set(vc.head(20).index)\n",
        "        qr_df[col] = qr_df[col].astype(str).where(qr_df[col].astype(str).isin(keep), other='OTHER')\n",
        "    # Build design matrix\n",
        "    X_raw = qr_df[['male01','hours','job_level','is_salary','department','job_family']].copy()\n",
        "    X_raw['hours2'] = (pd.to_numeric(X_raw['hours'], errors='coerce')**2)\n",
        "    X = pd.get_dummies(X_raw, drop_first=True)\n",
        "    X = X.apply(pd.to_numeric, errors='coerce')\n",
        "    y = pd.to_numeric(qr_df['log_annual'], errors='coerce')\n",
        "    # Drop any rows with NA after coercion and cast to float\n",
        "    mask = y.notna() & X.notna().all(axis=1)\n",
        "    X = X.loc[mask].astype(float)\n",
        "    y = y.loc[mask].astype(float)\n",
        "    X = sm.add_constant(X)\n",
        "    for tau in [0.25, 0.5, 0.75]:\n",
        "        res = QuantReg(y, X).fit(q=tau, max_iter=5000)\n",
        "        coef = res.params.get('male01', np.nan)\n",
        "        print('Quantile {:.2f}: male effect = {:.2f}%'.format(tau, (np.exp(coef)-1)*100))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Balance (ASMD): compute standardized mean differences pre- and post-weight; tabulate results.\n",
        "import numpy as np, pandas as pd\n",
        "# Recompute pre-PS SMDs (unweighted)\n",
        "def smd_numeric(x, g):\n",
        "    x1 = x[g==1].astype(float); x0 = x[g==0].astype(float)\n",
        "    m1, m0 = x1.mean(), x0.mean()\n",
        "    v1, v0 = x1.var(ddof=1), x0.var(ddof=1)\n",
        "    sp = np.sqrt((v1+v0)/2.0)\n",
        "    return float(abs(m1-m0)/sp) if sp>0 else 0.0\n",
        "def smd_binary(x, g):\n",
        "    p1 = x[g==1].mean(); p0 = x[g==0].mean()\n",
        "    p = 0.5*(p1+p0)\n",
        "    sp = np.sqrt(p*(1-p))\n",
        "    return float(abs(p1-p0)/sp) if sp>0 else 0.0\n",
        "# Weighted SMDs (ATE weights from 'work' DataFrame built earlier)\n",
        "def wmean(x, w): return float((w*x).sum() / w.sum())\n",
        "def wvar(x, w):\n",
        "    mu = wmean(x, w)\n",
        "    return float(((w*(x-mu)**2).sum() / w.sum()))\n",
        "def w_smd_numeric(x, g, w):\n",
        "    w1 = w[g==1]; x1 = x[g==1].astype(float)\n",
        "    w0 = w[g==0]; x0 = x[g==0].astype(float)\n",
        "    m1, m0 = wmean(x1, w1), wmean(x0, w0)\n",
        "    v1, v0 = wvar(x1, w1), wvar(x0, w0)\n",
        "    sp = np.sqrt((v1+v0)/2.0)\n",
        "    return float(abs(m1-m0)/sp) if sp>0 else 0.0\n",
        "def w_smd_binary(x, g, w):\n",
        "    p1 = wmean(x[g==1], w[g==1]); p0 = wmean(x[g==0], w[g==0])\n",
        "    p = 0.5*(p1+p0); sp = np.sqrt(p*(1-p))\n",
        "    return float(abs(p1-p0)/sp) if sp>0 else 0.0\n",
        "# Pre (gpd) and post (work) datasets\n",
        "pre = gpd.copy()\n",
        "post = work.copy() if 'work' in globals() else None\n",
        "has_weights = (post is not None) and ('w' in post.columns)\n",
        "# Compute ASMD pre and post\n",
        "covs = ['hours','job_level','is_full_time','is_salary']\n",
        "rows = []\n",
        "for c in covs:\n",
        "    if c in ['hours','job_level']:\n",
        "        pre_s = smd_numeric(pre[c].astype(float), pre['male01'])\n",
        "    else:\n",
        "        pre_s = smd_binary(pre[c].astype(float), pre['male01'])\n",
        "    if has_weights:\n",
        "        if c in ['hours','job_level']:\n",
        "            post_s = w_smd_numeric(post[c].astype(float), post['male01'], post['w'])\n",
        "        else:\n",
        "            post_s = w_smd_binary(post[c].astype(float), post['male01'], post['w'])\n",
        "    else:\n",
        "        post_s = np.nan\n",
        "    rows.append({'covariate': c, 'asmd_pre': pre_s, 'asmd_post_weighted': post_s})\n",
        "asmd_tbl = pd.DataFrame(rows).sort_values('asmd_post_weighted')\n",
        "if not has_weights:\n",
        "    print('Skipping post-weight ASMD: weights not available (propensity skipped).')\n",
        "display(asmd_tbl)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model comparison: aggregate effects across models (unadj/adj/FE/weighted/quantile); display and save to artifacts.\n",
        "import numpy as np, pandas as pd\n",
        "def eff_ci(res, name='male01'):\n",
        "    try:\n",
        "        est = res.params.get(name, np.nan)\n",
        "        ci = res.conf_int().loc[name]\n",
        "        lo, hi = ci[0], ci[1]\n",
        "    except Exception:\n",
        "        est, lo, hi = np.nan, np.nan, np.nan\n",
        "    to_pct = lambda b: (np.exp(b)-1)*100.0\n",
        "    return to_pct(est), to_pct(lo), to_pct(hi)\n",
        "\n",
        "rows=[]\n",
        "# Choose available model set\n",
        "models_list = None\n",
        "if 'm1_ols' in globals():\n",
        "    models_list = [('M1 Unadjusted', m1_ols), ('M2 +Hours+Job', m2_ols), ('M3 +Dept+Family FE', m3_ols), ('M4 +Interactions', m4_ols)]\n",
        "elif 'm1' in globals():\n",
        "    models_list = [('M1 Unadjusted', m1), ('M2 +Hours+Job', m2), ('M3 +Dept+Family FE', m3), ('M4 +Interactions', m4)]\n",
        "else:\n",
        "    models_list = []\n",
        "\n",
        "# Prefer robust extractor if available\n",
        "use_effect = effect_pct_from_model if 'effect_pct_from_model' in globals() else eff_ci\n",
        "for label,mod in models_list:\n",
        "    e,lo,hi = use_effect(mod, 'male01')\n",
        "    rows.append({'model':label, 'effect_pct': e, 'ci_lo': lo, 'ci_hi': hi})\n",
        "\n",
        "# Weighted FE (wres) if available\n",
        "try:\n",
        "    e = (np.exp(wres.params['male01'])-1)*100\n",
        "    lo = (np.exp(wres.conf_int().loc['male01',0])-1)*100\n",
        "    hi = (np.exp(wres.conf_int().loc['male01',1])-1)*100\n",
        "    rows.append({'model':'WLS (ATE-weighted) +FE', 'effect_pct': e, 'ci_lo': lo, 'ci_hi': hi})\n",
        "except Exception:\n",
        "    pass\n",
        "# Quantile tau=0.5 if large enough\n",
        "if len(gpd) >= 200:\n",
        "    from statsmodels.regression.quantile_regression import QuantReg\n",
        "    qr_df = gpd.copy()\n",
        "    # Reduce cardinality and coerce to strings for dummies\n",
        "    for col in ['department','job_family']:\n",
        "        vc = qr_df[col].astype(str).value_counts(dropna=False)\n",
        "        keep = set(vc.head(20).index)\n",
        "        qr_df[col] = qr_df[col].astype(str).where(qr_df[col].astype(str).isin(keep), other='OTHER')\n",
        "    X_raw = qr_df[['male01','hours','job_level','is_full_time','department','job_family']].copy()\n",
        "    X = pd.get_dummies(X_raw, drop_first=True)\n",
        "    X = X.apply(pd.to_numeric, errors='coerce')\n",
        "    y = pd.to_numeric(qr_df['log_annual'], errors='coerce')\n",
        "    mask = y.notna() & X.notna().all(axis=1)\n",
        "    X = sm.add_constant(X.loc[mask].astype(float))\n",
        "    y = y.loc[mask].astype(float)\n",
        "    try:\n",
        "        qr = QuantReg(y, X).fit(q=0.5, max_iter=5000)\n",
        "        coef = qr.params.get('male01', np.nan)\n",
        "        rows.append({'model':'Quantile (tau=0.5) +FE', 'effect_pct': (np.exp(coef)-1)*100, 'ci_lo': np.nan, 'ci_hi': np.nan})\n",
        "    except Exception:\n",
        "        pass\n",
        "cmp = pd.DataFrame(rows)\n",
        "display(cmp)\n",
        "# Save model comparison\n",
        "import os\n",
        "os.makedirs('artifacts', exist_ok=True)\n",
        "cmp.to_csv('artifacts/model_comparison.csv', index=False)\n",
        "try:\n",
        "    from tabulate import tabulate\n",
        "    open('artifacts/model_comparison.md','w').write(tabulate(cmp, headers='keys', tablefmt='github', showindex=False))\n",
        "except Exception:\n",
        "    open('artifacts/model_comparison.md','w').write(cmp.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Robustness: Ridge regression (numeric/categorical pipeline)\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.linear_model import RidgeCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Build male01 from gender_guess (df does not have male01 column)\n",
        "base = df.select(['log_annual','gender_guess','department','job_family','job_level','hours','is_salary']).drop_nulls().to_pandas()\n",
        "base['male01'] = (base['gender_guess'].astype(str) == 'MALE').astype(int)\n",
        "X = base[['male01','department','job_family','job_level','hours','is_salary']]\n",
        "y = base['log_annual'].values\n",
        "pre = ColumnTransformer([\n",
        "    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), ['department','job_family']),\n",
        "    ('num', StandardScaler(), ['male01','job_level','hours','is_salary'])\n",
        "])\n",
        "ridge = RidgeCV(alphas=[0.1,1.0,10.0], cv=5)\n",
        "pipe = Pipeline(steps=[('pre', pre), ('model', ridge)])\n",
        "pipe.fit(X, y)\n",
        "print('Ridge alpha selected:', pipe.named_steps['model'].alpha_)\n",
        "# Counterfactual prediction at mean profile: male01=1 vs 0\n",
        "X_mean = pd.DataFrame({\n",
        "    'male01':[0,1],\n",
        "    'department':[X['department'].mode()[0]]*2,\n",
        "    'job_family':[X['job_family'].mode()[0]]*2,\n",
        "    'job_level':[X['job_level'].mean()]*2,\n",
        "    'hours':[X['hours'].mean()]*2,\n",
        "    'is_salary':[X['is_salary'].mode()[0]]*2,\n",
        "})\n",
        "pred = pipe.predict(X_mean)\n",
        "print('Ridge-implied male effect (at means): {:.2f}%'.format((np.exp(pred[1]-pred[0])-1)*100))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GLM: Gamma family with log link (robust to heteroskedasticity on positive outcome)\n",
        "import numpy as np, pandas as pd\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.formula.api as smf\n",
        "from patsy import bs\n",
        "from statsmodels.genmod.families import Gamma\n",
        "from statsmodels.genmod.families.links import Log\n",
        "\n",
        "glm_df = gpd.copy()  # uses MALE/FEMALE only subset with log_annual available\n",
        "# Use level outcome with log link; reduce cardinality and avoid spline collinearity\n",
        "try:\n",
        "    glm_df = glm_df.dropna(subset=['annualized_pay','hours','job_level','is_salary','department','job_family']).copy()\n",
        "    # Reduce high-cardinality categoricals to top N to avoid singularities\n",
        "    for col in ['department','job_family']:\n",
        "        vc = glm_df[col].astype(str).value_counts(dropna=False)\n",
        "        keep = set(vc.head(20).index)\n",
        "        glm_df[col] = glm_df[col].astype(str).where(glm_df[col].astype(str).isin(keep), other='OTHER')\n",
        "    # Simpler polynomial hours terms for numerical stability\n",
        "    glmr = smf.glm('annualized_pay ~ male01 + hours + I(hours**2) + job_level + C(department) + C(job_family)',\n",
        "                    data=glm_df, family=Gamma(link=Log())).fit(cov_type='HC1', maxiter=200)\n",
        "    b = glmr.params.get('male01', np.nan)\n",
        "    ci = glmr.conf_int().loc['male01'].tolist() if 'male01' in glmr.params.index else [np.nan, np.nan]\n",
        "    e, lo, hi = (np.exp(b)-1)*100, (np.exp(ci[0])-1)*100, (np.exp(ci[1])-1)*100\n",
        "    print('GLM (Gamma log-link, HC1): male effect = {:.2f}% (95% CI {:.2f}% to {:.2f}%)'.format(e, lo, hi))\n",
        "    # Cluster-robust by department\n",
        "    dept_codes = glm_df['department'].astype('category').cat.codes.values\n",
        "    glmr_cl = glmr.get_robustcov_results(cov_type='cluster', groups=dept_codes)\n",
        "    b = glmr_cl.params.item(glmr_cl.params.index.get_loc('male01')) if hasattr(glmr_cl.params,'index') else glmr_cl.params[list(glmr_cl.params.index).index('male01')]\n",
        "    ci = glmr_cl.conf_int().loc['male01']\n",
        "    e, lo, hi = (np.exp(b)-1)*100, (np.exp(ci[0])-1)*100, (np.exp(ci[1])-1)*100\n",
        "    print('GLM (Gamma log-link, cluster by dept): male effect = {:.2f}% (95% CI {:.2f}% to {:.2f}%)'.format(e, lo, hi))\n",
        "except Exception as e:\n",
        "    try:\n",
        "        # Fallback to ridge-regularized GLM to resolve singularities; standardize numeric terms for stability\n",
        "        glm_df = glm_df.copy()\n",
        "        for col in ['hours','job_level']:\n",
        "            x = pd.to_numeric(glm_df[col], errors='coerce')\n",
        "            glm_df[col+'_z'] = (x - x.mean())/x.std(ddof=0) if x.std(ddof=0) > 0 else x\n",
        "        glmr = smf.glm('annualized_pay ~ male01 + hours_z + I(hours_z**2) + job_level_z + C(department) + C(job_family)',\n",
        "                       data=glm_df, family=Gamma(link=Log())).fit_regularized(alpha=1e-3, L1_wt=0.0, maxiter=5000, cnvrg_tol=1e-4)\n",
        "        b = glmr.params.get('male01', np.nan) if hasattr(glmr, 'params') else np.nan\n",
        "        e = (np.exp(b)-1)*100 if np.isfinite(b) else np.nan\n",
        "        print('GLM (Gamma log-link, ridge-regularized): male effect = {:.2f}%'.format(e))\n",
        "    except Exception as e2:\n",
        "        print('GLM failed:', e2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Doubly Robust ATE (AIPW / DML) with cross-fitting\n",
        "import numpy as np, pandas as pd, json, os\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.base import clone\n",
        "\n",
        "# Optional: accelerate nuisance models with PyTorch MPS (Apple GPU) if available\n",
        "import os as _dml_os\n",
        "USE_TORCH = bool(globals().get('HAS_MPS', False) and _dml_os.environ.get('USE_MPS_DML','0') == '1')\n",
        "if USE_TORCH:\n",
        "    try:\n",
        "        import torch\n",
        "        import torch.nn as nn\n",
        "        import torch.optim as optim\n",
        "        DEVICE = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
        "    except Exception:\n",
        "        USE_TORCH = False\n",
        "\n",
        "base = gpd[['log_annual','male01','department','job_family','job_level','hours','is_salary']].dropna().copy()\n",
        "y = base['log_annual'].values\n",
        "t = base['male01'].astype(int).values\n",
        "X = base[['department','job_family','job_level','hours','is_salary']].copy()\n",
        "pre = ColumnTransformer([('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), ['department','job_family']),\n",
        "                           ('num', 'passthrough', ['job_level','hours','is_salary'])])\n",
        "\n",
        "skf = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)\n",
        "psi = np.zeros(len(base))\n",
        "p_all = np.zeros(len(base))\n",
        "fold_idx = np.zeros(len(base), dtype=int)\n",
        "for k, (tr, te) in enumerate(skf.split(X, t)):\n",
        "    if USE_TORCH:\n",
        "        # Fit preprocessor on training fold only\n",
        "        pre_k = clone(pre)\n",
        "        Xtr_enc = pre_k.fit_transform(X.iloc[tr])\n",
        "        Xte_enc = pre_k.transform(X.iloc[te])\n",
        "        Xtr_enc = np.asarray(Xtr_enc, dtype=np.float32)\n",
        "        Xte_enc = np.asarray(Xte_enc, dtype=np.float32)\n",
        "\n",
        "        # Simple MLP architectures\n",
        "        class MLP(nn.Module):\n",
        "            def __init__(self, in_dim, out_dim=1):\n",
        "                super().__init__()\n",
        "                self.net = nn.Sequential(\n",
        "                    nn.Linear(in_dim, 128), nn.ReLU(),\n",
        "                    nn.Linear(128, 64), nn.ReLU(),\n",
        "                    nn.Linear(64, out_dim)\n",
        "                )\n",
        "            def forward(self, x):\n",
        "                return self.net(x)\n",
        "\n",
        "        def fit_mlp_reg(Xn, yn, epochs=50):\n",
        "            model = MLP(Xn.shape[1], 1).to(DEVICE)\n",
        "            opt = optim.Adam(model.parameters(), lr=1e-3)\n",
        "            lossf = nn.MSELoss()\n",
        "            xb = torch.from_numpy(Xn).to(DEVICE)\n",
        "            yb = torch.from_numpy(yn.astype(np.float32)).view(-1,1).to(DEVICE)\n",
        "            model.train()\n",
        "            for _ in range(epochs):\n",
        "                opt.zero_grad(); pred = model(xb); loss = lossf(pred, yb); loss.backward(); opt.step()\n",
        "            model.eval()\n",
        "            return model\n",
        "\n",
        "        def fit_mlp_clf(Xn, tn, epochs=50):\n",
        "            model = MLP(Xn.shape[1], 1).to(DEVICE)\n",
        "            opt = optim.Adam(model.parameters(), lr=1e-3)\n",
        "            lossf = nn.BCEWithLogitsLoss()\n",
        "            xb = torch.from_numpy(Xn).to(DEVICE)\n",
        "            yb = torch.from_numpy(tn.astype(np.float32)).view(-1,1).to(DEVICE)\n",
        "            model.train()\n",
        "            for _ in range(epochs):\n",
        "                opt.zero_grad(); logits = model(xb); loss = lossf(logits, yb); loss.backward(); opt.step()\n",
        "            model.eval()\n",
        "            return model\n",
        "\n",
        "        # Propensity\n",
        "        clf = fit_mlp_clf(Xtr_enc, t[tr].astype(np.float32))\n",
        "        with torch.no_grad():\n",
        "            p_hat = torch.sigmoid(clf(torch.from_numpy(Xte_enc).to(DEVICE))).cpu().numpy().ravel()\n",
        "        p_hat = np.clip(p_hat, 0.01, 0.99)\n",
        "        # Outcome models\n",
        "        rf1 = fit_mlp_reg(Xtr_enc[t[tr]==1], y[tr][t[tr]==1].astype(np.float32))\n",
        "        rf0 = fit_mlp_reg(Xtr_enc[t[tr]==0], y[tr][t[tr]==0].astype(np.float32))\n",
        "        with torch.no_grad():\n",
        "            m1 = rf1(torch.from_numpy(Xte_enc).to(DEVICE)).cpu().numpy().ravel()\n",
        "            m0 = rf0(torch.from_numpy(Xte_enc).to(DEVICE)).cpu().numpy().ravel()\n",
        "    else:\n",
        "        # Propensity model (logistic)\n",
        "        lr = Pipeline(steps=[('pre', clone(pre)), ('clf', LogisticRegression(max_iter=5000, solver='lbfgs'))])\n",
        "        lr.fit(X.iloc[tr], t[tr])\n",
        "        p_hat = lr.predict_proba(X.iloc[te])[:,1]\n",
        "        p_hat = np.clip(p_hat, 0.01, 0.99)\n",
        "        # Outcome models (T-learner with RF)\n",
        "        rf1 = Pipeline(steps=[('pre', clone(pre)), ('rf', RandomForestRegressor(n_estimators=400, random_state=42, n_jobs=-1))])\n",
        "        rf0 = Pipeline(steps=[('pre', clone(pre)), ('rf', RandomForestRegressor(n_estimators=400, random_state=42, n_jobs=-1))])\n",
        "        rf1.fit(X.iloc[tr][t[tr]==1], y[tr][t[tr]==1])\n",
        "        rf0.fit(X.iloc[tr][t[tr]==0], y[tr][t[tr]==0])\n",
        "        m1 = rf1.predict(X.iloc[te])\n",
        "        m0 = rf0.predict(X.iloc[te])\n",
        "    psi_te = m1 - m0 + t[te]*(y[te] - m1)/p_hat - (1 - t[te])*(y[te] - m0)/(1 - p_hat)\n",
        "    psi[te] = psi_te\n",
        "    p_all[te] = p_hat\n",
        "    fold_idx[te] = k\n",
        "ate = float(np.mean(psi))\n",
        "se = float(np.std(psi, ddof=1)/np.sqrt(len(psi)))\n",
        "ci = (ate - 1.96*se, ate + 1.96*se)\n",
        "ate_pct = (np.exp(ate)-1)*100.0\n",
        "ci_pct = ((np.exp(ci[0])-1)*100.0, (np.exp(ci[1])-1)*100.0)\n",
        "print('DML ATE on log(pay): {:.6f} (95% CI {:.6f} to {:.6f})'.format(ate, ci[0], ci[1]))\n",
        "print('DML ATE in % terms: {:.2f}% (95% CI {:.2f}% to {:.2f}%)'.format(ate_pct, ci_pct[0], ci_pct[1]))\n",
        "# Trimmed ATE (PS in [0.05,0.95])\n",
        "mask = (p_all >= 0.05) & (p_all <= 0.95)\n",
        "psi_trim = psi[mask]\n",
        "ate_t = float(np.mean(psi_trim))\n",
        "se_t = float(np.std(psi_trim, ddof=1)/np.sqrt(len(psi_trim)))\n",
        "ci_t = (ate_t - 1.96*se_t, ate_t + 1.96*se_t)\n",
        "ate_t_pct = (np.exp(ate_t)-1)*100.0\n",
        "ci_t_pct = ((np.exp(ci_t[0])-1)*100.0, (np.exp(ci_t[1])-1)*100.0)\n",
        "print('DML (trimmed) ATE in % terms: {:.2f}% (95% CI {:.2f}% to {:.2f}%)'.format(ate_t_pct, ci_t_pct[0], ci_t_pct[1]))\n",
        "# Save summary\n",
        "os.makedirs('artifacts', exist_ok=True)\n",
        "open('artifacts/dml_ate_summary.json','w').write(json.dumps({\n",
        "    'ate_log': ate, 'ci_log': ci, 'ate_pct': ate_pct, 'ci_pct': ci_pct,\n",
        "    'ate_pct_trim': ate_t_pct, 'ci_pct_trim': ci_t_pct\n",
        "}, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Robust Linear Model (HuberT) with FE; robust to heavy tails and outliers\n",
        "import statsmodels.api as sm, statsmodels.formula.api as smf\n",
        "\n",
        "try:\n",
        "    # Use explicit polynomial terms for hours to avoid reliance on unavailable poly()\n",
        "    rlm_res = smf.rlm('log_annual ~ male01 + hours + I(hours**2) + I(hours**3) + job_level + C(department) + C(job_family)', data=gpd, M=sm.robust.norms.HuberT()).fit()\n",
        "    b = rlm_res.params.get('male01', float('nan'))\n",
        "    ci = rlm_res.conf_int().loc['male01'] if 'male01' in rlm_res.params.index else [float('nan'), float('nan')]\n",
        "    print('RLM (HuberT): male effect = {:.2f}% (95% CI {:.2f}% to {:.2f}%)'.format((np.exp(b)-1)*100, (np.exp(ci[0])-1)*100, (np.exp(ci[1])-1)*100))\n",
        "except Exception as e:\n",
        "    print('RLM failed:', e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save consolidated results summary to JSON for auto-markdown\n",
        "import json, math, os\n",
        "os.makedirs('artifacts', exist_ok=True)\n",
        "\n",
        "def pct_from_log(b):\n",
        "    try:\n",
        "        return (math.exp(float(b)) - 1.0) * 100.0\n",
        "    except Exception:\n",
        "        return float('nan')\n",
        "\n",
        "def pack(model, name='male01'):\n",
        "    try:\n",
        "        b = float(model.params.get(name, float('nan')))\n",
        "        ci = model.conf_int().loc[name].tolist() if name in model.params.index else [float('nan'), float('nan')]\n",
        "        return {'coef_log': b, 'effect_pct': pct_from_log(b), 'ci_pct': [pct_from_log(ci[0]), pct_from_log(ci[1])]} \n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "hours_basis = globals().get('used', globals().get('hours_basis', None))\n",
        "fe_deg = globals().get('deg_used', None)\n",
        "fe_ols = globals().get('m3', globals().get('m3_ols', None))\n",
        "rlm = globals().get('rlm_res', None)\n",
        "glm = globals().get('glmr', None)\n",
        "ate_wls = globals().get('wres', None)\n",
        "ate_wls_trim = globals().get('wres_trim', None)\n",
        "\n",
        "res_json = {\n",
        "    'data': 'data/chicago_salaries.csv',\n",
        "    'hours_basis': str(hours_basis) if hours_basis is not None else None,\n",
        "    'fe_deg': int(fe_deg) if isinstance(fe_deg, (int, float)) else None,\n",
        "    'shap': globals().get('SHAP_INFO', None),\n",
        "    'models': {\n",
        "        'fe_ols': pack(fe_ols),\n",
        "        'rlm': pack(rlm),\n",
        "        'glm': pack(glm),\n",
        "        'ate_wls': pack(ate_wls),\n",
        "        'ate_wls_trim': pack(ate_wls_trim),\n",
        "    }\n",
        "}\n",
        "with open('artifacts/results_summary.json','w') as fh:\n",
        "    json.dump(res_json, fh, indent=2)\n",
        "print('Wrote artifacts/results_summary.json')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Render auto-updating markdown summary from results_summary.json\n",
        "import json, math\n",
        "try:\n",
        "    from IPython.display import Markdown, display\n",
        "except Exception:\n",
        "    Markdown = None\n",
        "    def display(x):\n",
        "        pass\n",
        "\n",
        "def fmt_ci(lo, hi):\n",
        "    try:\n",
        "        return f\"{float(lo):.2f}% to {float(hi):.2f}%\"\n",
        "    except Exception:\n",
        "        return 'n/a'\n",
        "\n",
        "with open('artifacts/results_summary.json') as fh:\n",
        "    R = json.load(fh)\n",
        "\n",
        "md = []\n",
        "md.append('# Chicago Salaries: Final Summary (Auto-Generated)')\n",
        "md.append('')\n",
        "md.append(f\"- Data: `{R.get('data','n/a')}`  \")\n",
        "md.append(f\"- Hours basis: `{R.get('hours_basis','n/a')}`  \")\n",
        "md.append(f\"- FE hours degree: `{R.get('fe_deg','n/a')}`  \")\n",
        "sh = R.get('shap')\n",
        "if sh:\n",
        "    md.append(f\"- SHAP model: `{sh.get('model','n/a')}` on {sh.get('n_eval','?')} rows  \")\n",
        "md.append('')\n",
        "\n",
        "M = R.get('models', {})\n",
        "for label, key in [('Within-role FE OLS (HC1)','fe_ols'),\n",
        "                   ('RLM (HuberT)','rlm'),\n",
        "                   ('GLM (Gamma log link)','glm'),\n",
        "                   ('ATE weighted FE','ate_wls'),\n",
        "                   ('ATE trimmed FE','ate_wls_trim')]:\n",
        "    v = M.get(key)\n",
        "    if v:\n",
        "        md.append(f\"**{label}**  \")\n",
        "        md.append(f\"- Male effect (%): {v.get('effect_pct','n/a'):.2f}%  \")\n",
        "        ci = v.get('ci_pct', [None,None])\n",
        "        md.append(f\"- 95% CI: {fmt_ci(ci[0], ci[1])}\")\n",
        "        md.append('')\n",
        "\n",
        "md.append('**Interpretation**')\n",
        "md.append('- Within-role FE OLS is the primary like-for-like estimate. RLM and GLM corroborate under heavy tails/heteroskedasticity.')\n",
        "md.append('- The ATE complements FE OLS by balancing men and women on observables across the workforce; trimmed ATE reports an overlap-safe effect.')\n",
        "md.append('- Hours effects use within-group centering + orthogonal basis to control multicollinearity; VIF and rank checks enforce a stable design.')\n",
        "\n",
        "# Expanded, reader-friendly narrative\n",
        "md.append('')\n",
        "md.append('## What The Results Mean')\n",
        "md.append('- Raw pay distributions show a long right tail (a few very high earners). The log scale makes the overall shape easier to compare: see `artifacts/pay_distribution_hist.png`.')\n",
        "md.append('- Men and women are not spread evenly across departments and job families (e.g., more men in POLICE and FIRE). This “who works where” pattern matters for pay: see `artifacts/top_categories_bars.png`.')\n",
        "md.append('- “Like-for-like” regression with department and job-family fixed effects (plus job level and hours terms) estimates the male effect on log pay near zero (well under 1%). With HC1 and department-clustered SEs, the effect is at most a few tenths of a percent — small in magnitude and borderline in significance.')\n",
        "md.append('- Across the whole workforce (not holding roles constant), ATE weighting indicates men earn about 7–8% more on average. That gap reflects differences in role/department mix rather than pay within the same role.')\n",
        "md.append('- Robust (Huber) and quantile models agree that outliers and tails do not drive the within-role finding; median effects are near zero.')\n",
        "md.append('- Model diagnostics flag heteroskedasticity and some nonlinearity, so we rely on robust standard errors and flexible hours terms. Independence/statistical rank conditions are satisfied after de-duplication of collinear terms.')\n",
        "\n",
        "md.append('## Visuals — How to Read Them')\n",
        "md.append('- Pay Histograms: Raw vs log-scale. Log-scale makes typical pay differences more interpretable because it compresses very high values (`artifacts/pay_distribution_hist.png`).')\n",
        "md.append('- Top Departments/Job Families: Bars show where people work the most; composition differences explain much of the raw gap (`artifacts/top_categories_bars.png`).')\n",
        "md.append('- SHAP Summary: Global feature importance for predicting log pay. Hours, job level, and department/family dummies dominate; gender has low global importance (`artifacts/shap_summary_beeswarm.png`, `artifacts/shap_importance_bar.png`).')\n",
        "md.append('- Influence & Residual Checks: Outlier and assumption diagnostics confirm robust inferences are appropriate (`artifacts/influence_scatter_clean.png`).')\n",
        "\n",
        "md.append('## Bottom Line (Plain English)')\n",
        "md.append('- There is a large raw difference in average pay between men and women in the City of Chicago workforce. But once we compare men and women doing the same type of work in the same departments (controlling for job family, department, job level, and hours), the remaining gender difference in pay is extremely small — under 1% — and not economically meaningful.')\n",
        "md.append('- The across-workforce gap (~7–8%) is best explained by who works where and in which roles. In short, the data support the conclusion that role and department mix drive the observed average gap; within the same role and department, pay is effectively the same by gender.')\n",
        "md.append('- Caveat: Gender is inferred from first names (some ambiguity), and we analyze one dataset. Even so, multiple robust models point to the same conclusion.')\n",
        "\n",
        "content = \"\\n\".join(md)\n",
        "try:\n",
        "    if Markdown is not None:\n",
        "        display(Markdown(content))\n",
        "    else:\n",
        "        raise ImportError('IPython not available')\n",
        "except Exception:\n",
        "    print(content)\n",
        "    import os\n",
        "    os.makedirs('artifacts', exist_ok=True)\n",
        "    open('artifacts/summary.md','w').write(content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Final Narrative Summary\n",
        "\n",
        "### What The Results Mean\n",
        "- Raw pay distributions are skewed with a long right tail; plotting log pay makes typical values clearer (`artifacts/pay_distribution_hist.png`).\n",
        "- Men and women are not spread evenly across departments and job families (e.g., more men in POLICE and FIRE). This composition matters for average pay differences (`artifacts/top_categories_bars.png`).\n",
        "- Like-for-like comparisons (same department and job family, controlling for job level and hours) show a remaining gender effect on log pay that is very small (well under 1%) and not economically meaningful. Robust and cluster-adjusted inferences agree.\n",
        "- Across the workforce (without holding roles constant), the average pay gap is larger (~7–8%) and is explained by where people work and which roles they hold, not different pay for the same role.\n",
        "- Robust (Huber) and quantile models indicate that outliers and tails do not drive the within-role finding; median effects are also near zero.\n",
        "- Diagnostics flag unequal variance and some nonlinearity, so we use robust standard errors and flexible hours terms. Independence and rank conditions are satisfied after reducing collinearity.\n",
        "\n",
        "### Visuals — How to Read Them\n",
        "- Pay Histograms: Raw vs log-scale; the log scale compresses extreme values and makes typical differences clearer (`artifacts/pay_distribution_hist.png`).\n",
        "- Top Departments/Job Families: Bars show where people work most; composition differences explain much of the raw gap (`artifacts/top_categories_bars.png`).\n",
        "- SHAP Summary: Global feature importance for predicting log pay; hours, job level, and department/family dominate; gender has low global importance (`artifacts/shap_summary_beeswarm.png`, `artifacts/shap_importance_bar.png`).\n",
        "- Influence & Residual Checks: Outlier/assumption diagnostics support using robust inferences (`artifacts/influence_scatter_clean.png`).\n",
        "\n",
        "### Bottom Line (Plain English)\n",
        "- There is a sizeable raw difference in average pay between men and women in the City workforce. But when we compare men and women doing the same type of work in the same departments, the remaining gender difference is effectively zero.\n",
        "- The across-workforce gap (~7–8%) is best explained by who works where and in which roles. In short, role and department mix drive the observed average gap; within the same role and department, pay is essentially the same by gender.\n",
        "- Caveat: Gender is inferred from first names (some ambiguity), and we analyze one dataset. Even so, multiple robust models point to the same conclusion.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Visuals (with brief explanations)\n",
        "\n",
        "**Pay histograms (raw and log)**\n",
        "![Pay distribution](artifacts/pay_distribution_hist.png)\n",
        "Raw scale shows a few very high earners; log scale makes typical pay clearer.\n",
        "\n",
        "**Top departments and job families (counts)**\n",
        "![Top categories](artifacts/top_categories_bars.png)\n",
        "Men and women are distributed differently across roles/departments; composition drives the raw gap.\n",
        "\n",
        "**SHAP summary (beeswarm)**\n",
        "![SHAP beeswarm](artifacts/shap_summary_beeswarm.png)\n",
        "Hours, job level, and department/family features dominate predicted pay; gender has low global importance.\n",
        "\n",
        "**SHAP importance (bar)**\n",
        "![SHAP importance](artifacts/shap_importance_bar.png)\n",
        "Mean absolute SHAP values confirm which features matter most overall.\n",
        "\n",
        "**Influence and residual diagnostics**\n",
        "![Influence diagnostics](artifacts/influence_scatter_clean.png)\n",
        "Highlights high-leverage/outlying points; supports using robust standard errors.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Glossary of Methods and Acronyms\n",
        "- FE (Fixed Effects): Controls for group differences (e.g., department, job family) so comparisons are like-for-like within groups.\n",
        "- HC1 robust SEs: Standard errors adjusted for unequal variance across observations.\n",
        "- Cluster-robust SEs: Standard errors that allow correlation within clusters (here, departments).\n",
        "- ATE (Average Treatment Effect): Average difference in outcome if everyone had the ‘treatment’ (male) vs ‘control’ (female), balancing observables.\n",
        "- Propensity score: Estimated probability of being in the treatment group given observed features; used to balance groups.\n",
        "- DML (Double/Debiased Machine Learning): Uses flexible models for nuisance parts (propensity/outcome) while keeping the final effect unbiased.\n",
        "- GLM (Gamma, log link): Model for positive, skewed outcomes; relates log(expected pay) to predictors.\n",
        "- RLM (Robust Linear Model, Huber): Regression that downweights outliers to protect estimates.\n",
        "- Quantile regression: Models medians (or other quantiles) instead of means; robust to skew/outliers.\n",
        "- SHAP (SHapley Additive exPlanations): Consistent feature importance values that explain model predictions.\n",
        "- VIF (Variance Inflation Factor): Indicator of multicollinearity; very high VIF suggests overlapping information between predictors.\n",
        "- RESET test: Checks if a linear model is missing nonlinear terms or interactions.\n",
        "- Durbin–Watson (DW): Tests residual autocorrelation; ≈2 suggests independence.\n",
        "- Breusch–Pagan (BP) / White tests: Tests for unequal variance (heteroskedasticity).\n",
        "- Oaxaca–Blinder: Decomposes the raw gap into explained (composition) and unexplained parts.\n",
        "- Orthogonal polynomials (Legendre): Curved terms for hours that avoid collinearity with other predictors.\n",
        "- 95% CI: Range likely containing the true effect; if it includes 0%, the effect is consistent with no difference.\n",
        "- Log pay: Using log(pay) turns percentage effects into additive terms and stabilizes variance.\n",
        "- MPS (Apple GPU): Apple’s GPU acceleration used for some ML steps (optional).\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv-polars",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
